\documentclass[10pt]{scrartcl}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}

\usepackage{multicol}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{cclicenses}
\usepackage{textcomp}

\setkomafont{section}{}
\setkomafont{subsection}{}
\setkomafont{subsubsection}{}
\setkomafont{paragraph}{}
\setkomafont{subparagraph}{\normalfont\itshape}

\usepackage[landscape,paper=a4paper,left=4mm,right=4mm,top=5mm,bottom=5mm,footskip=0mm]{geometry}
\usepackage[extreme]{savetrees}

\renewcommand\familydefault{\rmdefault}
\let\oldenumerate\enumerate
\renewcommand{\enumerate}{
	\oldenumerate
	\setlength{\itemsep}{0pt}
	\setlength{\parskip}{2pt plus1pt minus1pt}
	\setlength{\parsep}{0pt}
}
\let\olditemize\itemize
\renewcommand{\itemize}{
	\olditemize
	\setlength{\itemsep}{0pt}
	\setlength{\parskip}{2pt plus1pt minus1pt}
	\setlength{\parsep}{0pt}
}
\setlength{\parskip}{3pt plus2pt minus1pt}
\setlength{\parsep}{0pt}

\usepackage[]{titlesec} % SPACING überall definieren
\titlespacing*{\section}{0pt}{0pt}{0pt} %indent,space befor, space after
\titlespacing*{\subsection}{0pt}{0pt}{0pt}
\titlespacing*{\subsubsection}{0pt}{0pt}{0pt}
\titlespacing*{\paragraph}{0pt}{0pt}{5pt}
\titlespacing*{\subparagraph}{0pt}{0pt}{5pt}

%opening
\title{Data Mining (FS 2016)}
\author{Tim Taubner}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\Sim}{Sim}
\DeclareMathOperator{\Proj}{Proj}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\newcommand{\eps}{\epsilon}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\inuar}{\in_{\text{u.a.r.}}}

\setlength{\columnsep}{2mm}
\setlength{\jot}{1pt}

\begin{document}

\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
%\fancyfoot[L]{\footnotesize Data Mining Summary for exam use. HS 2016, \textcopyright Tim Taubner. Creative Commons 2.0 \byncsa}
%\fancyfoot[R]{\footnotesize \thepage}

\pagenumbering{roman}
\begin{centering}
\vspace*{1em}
\vfill
\textbf{Disclaimer} \\
I wrote this to my best knowledge, however, no guarantees are given whatsoever.
\vfill
\textbf{Sources} \\
If not noted differently, the source is the lecture slides and/or the accompanying book.
\vfill
\end{centering}

\pagenumbering{arabic}
\setcounter{page}{0}

\clearpage

\begin{multicols}{3}

\section{Approximate Retrieval}
\paragraph{Nearest-Neighbor} Find $\bm x^* = \argmin_{\bm x \in X} \ d(\bm x,\bm y)$
given $S,\ \bm y \in S,\ X\subseteq S$.
\paragraph{Near-Duplicate detection}
Find all $\bm x, \bm x'\in X$ with $d(\bm x,\bm x') \leq \eps$.
\subsection{$k$-Shingling}
Documents (or videos) as set of $k$-shingles (a.\ k.\ a.\ $k$-grams).
\emph{$k$-shingle} is consecutive appearance of $k$ chars/words. \\
Binary \emph{shingle matrix} $M \in \{0,1\}^{C \times N}$ where $M_{i,j} = 1$ iff document $j$ contains shingle $i$, $N$ documents, $C$ $k$-shingles.
\subsection{Distance functions}
\paragraph{Def.}
$d: S \times S \rightarrow \R$ is \emph{distance function} iff pos.\ definite except $d(x,x) = 0$ ($d(x,x') > 0 \iff x \neq x'$), symmetric ($d(x,x') = d(x',x)$) and triangle inequality holds ($d(x,x'') \leq d(x,x') + d(x',x'')$).
\paragraph{Euclidean $L_r$}
$d_r(x,y) = ||x-y||_r = (\sum_i |x_i - y_i|^r)^{1/r}$.
\paragraph{Cosine}
$\Sim_c(A,B) = \dfrac{A \cdot B}{||A||\cdot||B||},\ d_c(A,B) = \dfrac{\cos^{-1}(\Sim_c(A,B))}{\pi}.$
\paragraph{Jaccard sim., d.}
$\Sim_J(A,B) = \dfrac{|A \cap B|}{|A \cup B|},\ d_J(A,B) = 1 - \Sim_J(A,B).$
% Potentially edit distance and/or hamming distance

\subsection{LSH -- local sensitive hashing}
\emph{Key Idea:} Similiar documents have similiar hash. \\
\emph{Note:} Trivial for exact duplicates (hash-collision $\rightarrow$ candidate pair).
\paragraph{Min-hash-family $h_\pi(C)$ for Jaccard}
Hash is the \emph{min (i.e.\ first) non-zero permutated row index}: $h_\pi(C) = \min_{i, C(i) = 1} \pi(i)$,
bin.\ vec.\ $C$, rand.\ perm.\ $\pi$. \\
\emph{Note:} $\Pr_\pi[h_\pi(C_1) = h_\pi(C_2)] = \Sim_J(C_1,C_2)$ if $\pi \inuar S_{|C|}$.
\paragraph{Min-hash $L_r$-norm:}
Fix $a \in \R$.
Random line $\bm w$ paritioned in buckets of length $a$.
Project $\bm x, \bm y$ onto $\bm w$, if in same bucket, $h_{\bm w}(x) = h_{\bm w}(y)$.
In 2-dim. forms a $(a/2, 2\cdot a, 1/2, 1/3)$-sensitive hash-family.
In d-dim. there exists a $(d1, d2, p1, p2)$-sensitive family $\forall d1 < d2$ with $p1 > p2$.
\paragraph{Min-hash cos.} $h_{\bm w}(\bm x) = \sign(\bm w^T\bm x)$. $\Pr_{\bm w}[h_{\bm w}(x) = h_{\bm w}(y)] = 1 - \frac{\theta_{\bm x, \bm y}}{\pi}$.
\paragraph{Min-hash signature matrix $M_S \in [N]^{n\times C}$}
with $M_S(i,c) = h_i(C_c)$
given $n$ hash-fns $h_i$ drawn randomly from a universal hash family.

\paragraph{Pseudo permutation}
$h_\pi$ with $\pi(i) = (a\cdot i + b) \mod\ p \mod\ N$, $N$ number of shingles, $p\geq N$ prime and $a,b \inuar [p]$ with $a \neq 0$. \\
Use as universal hash family. Only store $a$ and $b$. Much more efficient.
% hash family for cosine: sign(w^Tv), from ex. 1

\paragraph{Compute signature matix $M_S$}
For column $c \in [C]$, row $r \in [N]$ with $C_c(r) = 1$, $M_S(i,c) \leftarrow \min\{h_i(C_c), M_S(i,c)\}$ for all $h_i$.

\paragraph{$(d_1,d_2,p_1,p_2)$-sensitivity} of $F = \{h_1,\dots,h_n\}$: $\forall x,y \in S: d(x,y) \leq d_1 \implies P[h(x)=h(y)] \geq p_1$ and $d(x,y) \geq d_2 \implies P[h(x)=h(y)] \leq p_2.$
\paragraph{$r$-way AND}
$h = [h_1,\dots,h_r],\ h(x) = h(y) \Leftrightarrow \forall i\ h_i(x) = h_i(y)$ is $(d_1,d_2,p_1^r,p_2^r)$-sensitive. Decreases FP.
\paragraph{$b$-way OR}
$h = [h_1,\dots,h_b],\ h(x) = h(y) \Leftrightarrow \exists i\ h_i(x) = h_i(y)$ is $(d_1,d_2,1-(1-p_1)^b,1-(1-p_2)^b)$-sensitive. Decreases FN.

\paragraph{Boosting by composition}
$b$-way OR after $r$-way AND.
Group sig.\ matrix into $b$ bands of $r$ rows.
CP match in at least one band (check by hashing).
Result is $(d_1,d_2,1-(1-p_1^r)^b,1-(1-p_2^r)^b)$-sensitive. $r$ pulls stronger than $b$. $r$ pulls down.
\\
$r$-way AND after $b$-way OR.
Result is $(d_1,d_2,(1-(1-p_1)^b)^r,(1-(1-p_2)^b)^r$-sensitive. $b$ pulls stronger than $r$. $b$ pulls up.

\paragraph{Tradeoff FP/FN}
Favor FP (work) over FN (wrong).
Filter FP by checking signature matrix, shingles or even whole documents.

% TODO: Tradeoff-formulae

\section{Supervised Learning}
\paragraph{Linear classifier} $y_i = \sign(\bm w^T\bm x_i)$ assuming $\bm w$ goes through origin.
\paragraph{Homogeneous transform} $\tilde{\bm x} = [\bm x, 1]; \tilde{\bm w} = [\bm w, b]$, now $\bm w$ passes origin.
\paragraph{Kernel}
$k$ is inner product in high-dim.\ space: $k(\bm x,\bm y) = \langle\phi(\bm x),\phi(\bm y)\rangle$.\\
%\emph{Kernel/Gram-matrix} .\\
\emph{shift-invariance} $k(\bm x,\bm y) = k(\bm x - \bm y)$. \\
\emph{Gaussian} $k(\bm x - \bm y) = \exp(-||\bm x - \bm y||_2^2/h^2)$.
% TODO: More about kernels

\paragraph{Convex function} $f: S \rightarrow \R$ is convex iff $\forall x,x'\in S, \lambda \in [0,1], \lambda f(x) + (1-\lambda)f(x') \geq f(\lambda x + (1-\lambda)x')$, i.\ e.\ every segment lies above function. Equiv.\ bounded by linear fn.\ at every point.
%\subparagraph{Concave} $f$ concave, iff $-f$ convex.
\subparagraph{$H$-strongly convex} $f$ $H$-strongly convex iff $f(x') \geq f(x) + \nabla f(x)^T(x'-x)+\frac{H}{2}||x'-x|_2^2$, i.\ e.\ bounded by quadratic fn (at every point).
% Bonus: Lagrangian
\subsection{Support vector machine (SVM)}
\paragraph{SVM primal}
\subparagraph{Quadratic} $\min_{\bm w} \bm w^T \bm w + C\sum_{i}\xi_i$,
s.t.\ $\forall i: y_i\bm w^T \bm x_i \geq 1 - \xi_i$, $\forall i: \xi_i\geq 0$.
\subparagraph{Hinge loss}
%$\min_{\bm w} \bm w^T \bm w + C\sum_{i}\max(0,1-y_i\bm w^T\bm x_i)$,\newline
%Also written
$\min_{\bm w} \lambda \bm w^T \bm w + \sum_{i}\max(0,1-y_i\bm w^T \bm x_i)$ with $\lambda = \frac{1}{C}$.
%$\min_{\bm w} \lambda \bm w^T \bm w + \sum_{i}l(\bm w; \bm x_i, y_i)$ with $\lambda = \frac{1}{C}$.
%With \emph{hinge loss} $l(\bm w; \bm x_i, y_i) = \max(0,1-y_i\bm w^T \bm x_i)$.
\subparagraph{Norm-constrained}
$\min_{\bm w} \sum_{i}\max(0,1-y_i\bm w^T\bm x_i)$ s.t.\ $||\bm w||_2 \leq \frac{1}{\sqrt \lambda}$.
\paragraph{Lagrangian dual}
$\max_{\bm \alpha} \sum_{i}\alpha_i + \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j \bm x_i^T\bm x_j,\ \alpha_i \in [0,C]$.
Apply kernel trick: 
$\max_{\bm \alpha} \sum_{i}\alpha_i + \frac{1}{2}\sum_{i,j}\alpha_i\alpha_j y_i y_j k(\bm x_i, \bm x_j),\ \alpha_i \in [0,C]$, prediction becomes $y = \sign(\sum_{i=1}^{n}\alpha_i y_i k(\bm x_i,\bm x))$.

\subsection{Convex Programming}
\paragraph{Convex program} $\min_{\bm x} f(\bm x)$, s.\ t.\ $\bm x \in S$, $f$ convex.
\paragraph{Online convex program (OCP)} $\min_{\bm w} \sum_{t=1}^{T} f_t(\bm w)$, s.\ t.\ $\bm w \in S$.
\paragraph{General regularized form} $\min_{\bm w} \sum_{i=1}^{n} l(\bm w; \bm x_i, y_i) + \lambda R(\bm w)$, where $l$ is a (convex) loss function and $R$ is the (convex) regularizer.
\paragraph{General norm-constrained form} $\min_{\bm w} \sum_{i=1}^{n} l(\bm w; \bm x_i, y_i)$, s.\ t.\ $\bm w \in S_\lambda$, $l$ is loss and $S_\lambda$ some (norm-)constraint. Note: This is an OCP.

\paragraph{Solving OCP}
\emph{Feasible set} $S \subseteq \R^d$ and \emph{start pt.} $\bm w_0 \in S$, OCP (as above).
Round $t \in [T]$: pick feasible pt.\ $\bm w_t$, get convex fn.\ $f_t$, incur $l_t = f_t(\bm w_t)$.
Regret $R_T = (\sum_{t=1}^{T} l_t) - \min_{\bm w \in S}\sum_{t=1}^{T}f_t(\bm w)$.

\paragraph{Online SVM}
$||\bm w||_2 \leq \frac{1}{\lambda}$ (norm-constr.). For new pt.\ $\bm x_t$ classify $y_t = \sign(\bm w_t^T \bm x_t)$,
incur $l_t = \max(0,1-y_t \bm w_t^T \bm x_t)$, update $\bm w_t$ (see later).
Best $L^* = \min_{\bm w}\sum_{t=1}^{T}\max(0,1-y_t \bm w^T \bm x_t)$, regret $R_t = \sum_{t=1}^{T} l_t - L^*$.

\paragraph{Online proj.\ gradient descent (OPGD)}
Update for online SVM: \\
$\bm w_{t+1} = \Proj_S(\bm w_t-\eta_t\nabla f_t(\bm w_t))$ with $\Proj_S(\bm w) = \argmin_{w'\in S} ||\bm w'-\bm w||_2$, gives regret bound $\frac{R_T}{T}\leq\frac{1}{\sqrt{T}}(||\bm w_0-\bm w^*||_2^2+||\nabla f||_2^2)$. \\
For $H$-strongly convex fn, $\eta_t = \frac{1}{Ht}$ gives $R_t \leq \frac{||\nabla f||^2}{2H}(1+\log T)$.

\paragraph{Stochastic Gradient Descent (SGD)}
Convex program is unconstrained and decomposes $f(w)=\sum_{i}^n{g(w; i)}$. In SVM $g(w; i) = \frac{\lambda w^Tw}{n} + l_h(w; x_i, y_i)$. \emph{Algo:} $w \leftarrow 0$ and choose $\beta_t$. until convergence: $(x_i,y_i)\in_{u.a.r}X$. $w\leftarrow w - \beta_t \cdot \nabla_w(g(w; i))$.

\paragraph{Stochastic PGD (SPGD)}
Online-to-batch. Compute $\tilde{\bm w} = \frac{1}{T}\sum_{t=1}^{T} \bm w_t$.
If data i.\ i.\ d.: exp.\ \emph{error (risk)} $\E[L(\tilde{\bm w})] \leq L(\bm w^*) + R_T/T$, $L(\bm w^*)$ is best error (risk) possible.

\paragraph{PEGASOS}
OPGD w/ mini-batches on strongly convex SVM form. \\
$\min_w \sum_{t=1}^{T} g_t(\bm w)$, s.t. $||\bm w||_2 \leq \frac{1}{\sqrt t}$, $g_t(\bm w) = \frac{\lambda}{2}||\bm w||_2^2 + f_t(\bm w)$. \\
$g_t$ is $\lambda$-strongly convex, $\nabla g_t(\bm w) = \lambda \bm w + \nabla f_t(\bm w)$. \\
\emph{Performance} $\eps$-accurate sol.\ with prob.\ $\geq 1-\delta$ in runtime $O^*(\frac{d\cdot\log\frac{1}{\delta}}{\lambda\eps}).$

\paragraph{ADAGrad}
Adapt to geometry.
\emph{Mahalanobis norm} $||\bm w||_{\bm G} = ||\bm G\bm w||_2$. \\
$\bm w_{t+1}=\argmin_{\bm w\in S}||\bm w - (\bm w_t - \eta \bm G_t^{-1}\nabla f_t(\bm w))||_{\bm G_t}$.
Min. regret with $G_t = (\sum_{\tau = 1}^{t} \nabla f_\tau(\bm w_\tau)\nabla f_\tau(\bm w_\tau)^T)^{1/2}.$
Easily inv'able matrix with $G_t = \diag(\dots).$
$R_t \in O(\frac{||\bm w^*||_\infty}{\sqrt T}\sqrt d)$, even better for sparse data.
%for sparse data (non-zero feature with prob.\  even
%$O(\frac{||\bm w^*||_\inf}{\sqrt T}\max\{\log d, d^{1-c/2} \})$
% Bonus: Pseudo-code

\paragraph{ADAM}
Add `momentum' term: $\bm w_{t+1} = \bm w_t - \mu\bar g_t$, $g_t = \nabla f_t(\bm w)$, $\bar g_t = (1-\beta)g_t + \beta \bar g_{t-1}$, $\bar g_0 = 0$.
% Bonus: Pseudo-code
Helps for dense gradients.

\paragraph{Parallel SGD (PSGD)}
Randomly partition to $k$ (indep.) machines. Comp.\ $\bm w = \frac{1}{k}\sum_{i=1}^k \bm w_i$.
$\E[\text{err}] \in O(\eps(\frac{1}{k\sqrt\lambda}+1))$ if $T \in \Omega(\frac{\log \frac{k\lambda}{\eps}}{\eps\lambda})$.
Suitable for MapReduce cluster, multi.\ passes possible.

\paragraph{Hogwild!}
Shared mem., no sync., sparse data. [\dots]

\paragraph{Implicit kernel trick}
Map $x \in \R^d \rightarrow \phi(x) \in \R^D \rightarrow z(x) \in \R^m$, $d \ll D, m \ll D$.
Where $\phi(x)$ corresponds to a kernel $k(x,x') = \phi(x)^T\phi(x')$.

\paragraph{Random fourier features}
Given shift-invariant kernel $k$.\\
$p(\omega)=\frac{1}{2\pi}\int e^{-j\omega'\delta}k(\delta) \diff\Delta$ \\
$\omega_i \sim p =$ eg Gaussian, $b_i \sim U(0, 2\pi)$\\
$\bm z(\bm x) \equiv \sqrt{2/m}[cos(\omega_1'\bm x+b_1) \dots cos(\omega_m'\bm x+b_m)]$ 

\paragraph{Nyström features (need entire dataset)}
In practice: pick random samples $S = \{\hat{\bm x}_1 \dots \hat{\bm x}_n\} \subseteq X$ \\
${\bm K_{SX}}_{i,j} = k(\hat{\bm x}_i, \hat{\bm x}_j)$, ${\bm K_{SS}}_{i,j} = k(\hat{\bm x}_i, \hat{\bm x}_j)$\\
approximate $\bm K = \bm K_{XS}\bm K_{SS}^{-1}\bm K_{SX}$,$\bm K_{SS}=\bm V \bm D \bm V^T$.\\
new point $\bm x'$: $\bm z(\bm x') = \bm D^{-1/2} \bm V^T [k(\bm x', \hat{\bm x}_1),...,k(\bm x', \hat{\bm x}_m)]$


\section{Pool-based active Learning (semi-supervised)}
%\paragraph{Stream-based*} Data point arrives online, decide if label needed.
%\paragraph{Pool-based} Unlabeled data-set given, (sequentially) request labels.

\paragraph{Uncertainty sampl.}
$U_t(x) = U(x | x_{1:t-1}, y_{1:t-1})$, request $y_t$ for $x_t = \argmax_x U_t(x)$.
\textbf{SVM:} $x_t = \argmin_{x_i}|\bm w^T\bm x_i|$, i.e.\ $U_t(\bm x) = \frac{1}{|\bm w_t^T \bm x|}$.

\paragraph{Sub-linear time w/ LSH}
$|\bm w^T\bm x_i|$ small if $\angle_{\bm w,\bm x_i}$ close to $\pi$.\\
Hash hyperplane:
$h_{\bm u,\bm v}(\bm a,\bm b) = [h_{\bm u}(\bm a), h_{\bm v}(\bm b)] = [\sign(\bm u^T\bm a),\sign(\bm v^T\bm b)]$.
LSH hash family: $h_H(z) = h_{u,v}(\bm z,\bm z)$ if $z$ datapoint, $h_H(z) = h_{u,v}(\bm z,-\bm z)$ if $z$ query hyperplane.
$\Pr[h_H(\bm w) = h_H(\bm x)] = \Pr[h_{\bm u}(\bm w) = h_{\bm u}(\bm x)]\Pr[h_{\bm v}(-\bm w) = h_{\bm v}(\bm x)] = \frac{1}{4} - \frac{1}{\pi^2}(\angle_{\bm x,\bm w} - \frac{pi}{2})^2$.\\
Hash all unlabeled. Loop: Hash $\bm w$, req.\ labels for hash-coll., update.

\paragraph{Informativeness}
Metric of “information” gainable; $\neq$ uncertainty.
\paragraph{Version Space}
$\mathcal{V}(D) = \{\bm w\ |\ \forall(\bm x,y)\in D\ \sign(\bm w^T\bm x) = y)\}$

\paragraph{Relevant version space} given unlabeled pool $U = \{x'_1,\dots,x'_n\}$.
$\tilde{\mathcal{V}}(D;U) = \{h: U\rightarrow\{\pm 1\}\ |\ \exists\bm w \in \mathcal{V}(D)\ \forall x\in U\ \sign(\bm w^T \bm x) = h(\bm x)\}.$

\paragraph{Generalized binary search}
Init $D \leftarrow \{\}$.
While $|\tilde{\mathcal{V}}(D;U)| > 1$, comp.\ $v^\pm(x) = |\tilde{\mathcal{V}}(D \cup \{(x,\pm)\};U)|$, label of $\argmin_x \max\{v^-(x),v^+(x)\}$.

\paragraph{Approx. $|\mathcal{V}|$}
Margins of SVM $m^\pm(x)$ for labels $\{+,-\}, \forall x$. \emph{Max-min} $\max_x \min\{m^+(x),m^-(x))\}$ or \emph{ratio} $\max_x \min\{\frac{m^+(x)}{m^-(x)},\frac{m^-(x)}{m^+(x)}\}$.

\section{Model-based clustering -- Unsupervised learning}
\paragraph{k-means problem}
$\min_\mu L(\mu)$ with
$L(\mu) = \sum_{i=1}^{N}\min_j||\bm x_i-\mu_j||_2^2$ and \emph{cluster centers} $\mu = \mu_1,\dots,\mu_k$.
Non-convex! NP-hard in general!

\paragraph{LLoyd's}
Init $\mu^{(0)}$ (somehow).
\emph{Assign} all $\bm x_i$ to closest center $z_i \leftarrow \argmin_{j \in [k]} ||\bm x_i - \mu_j^{(t-1)}||_2^2$,
\emph{Update} to mean: $\mu_j^{(t)} \leftarrow \frac{1}{n_j}\sum_{i:z_i = j}\bm x_i$.
Always converge to \emph{local minimum}.

\paragraph{Online k-means} Init $\mu$ somehow.
For $t \in [n]$ find $c = \argmin_j||\mu_j-\bm x_t||_2$, set $\mu_c \leftarrow \mu_c + \eta_t(\bm x_t - \mu_c)$. For local optimum: $\sum_t \eta_t = \infty \wedge \sum_t \eta_t^2 < \infty$ suffices, e.g.\ $\eta_t = \frac{c}{t}$, $c \in \R$.

\paragraph{Weighted rep. $C$} $L_k(\mu;C) = \sum_{(w,\bm x)\in C}w\cdot\min_j||\mu_j - \bm x||_2^2$.

\paragraph{$(k,\eps)$-coreset} iff $\forall\mu: (1-\eps)L_k(\mu;D) \leq L_k(\mu;C) \leq (1+\eps)L_k(\mu;D)$.

\paragraph{$D^2$-sampling}
Sample prob.\ $p(x) = \frac{d(x,B)^2}{\sum_{x' \in X} d(x', B)^2}$.

% Maybe include importance sampling?
%\paragraph{Step 2: Importance sampling}
%$B_i = \{\bm x\in D: i \in \argmin\ d(\bm x,b_i)\}$ ?? what is i ??
%Sample $q(\bm x) \propto \frac{\alpha d(\bm x,B)^2}{c_\phi} + \frac{2\alpha\sum_{\bm x'\in B_i} d(\bm x',B)^2}{|B_i|c_\phi} + \frac{4|X|}{|B_i|}$, $c_\phi = \frac{1}{|X|}\sum_{x\in X}d(x,B)^2$, $\alpha = \log_2 k + 1$.
%Coreset size $O(\frac{dk^3}{\eps^2})$.

\paragraph{Merge coresets} union of $(k,\eps)$-coreset is also $(k,\eps)$-coreset.

\paragraph{Compress} a $(k,\delta)$-coreset of a $(k,\eps)$-coreset is a $(k,\eps+\delta+\eps\delta)$-coreset.

\paragraph{Coresets on streams} Bin.\ tree of merge-compress. Error $\propto$ height.

\paragraph{Mapreduce k-means} Construct $(k,\eps)$-coreset C, solve k-means (w/ many restarts) on coreset. (Repeat.) Near-optimal solution.

\section{$k$-armed bandits as recommender systems}
\paragraph{$k$-armed bandit}
$k$ arms with diff.\ prob.\ dist. For $t \in [T]$ rounds, pick $i_t\in[k]$, sample $y_t \in P_i$ (indep.\ of other rounds). Max.\ $\sum_{t=1}^{T} y_t$.
\paragraph{Regret}
$\mu_i$ mean of $P_i$ (arm $i$), $\mu^* = \max_i \mu_i$.
Regret $r_t = \mu^* - \mu_{i_t}$.
Total regret $R_T = \sum_{t=1}^{T} r_t$.
\paragraph{$\eps$-greedy} \emph{Explore} u.a.r.\ with prob. $\eps_t$, \emph{exploit} with prob.\ $1-\eps_t$: choose $\argmax_i\hat\mu_i$.
Suitable $\eps_t \in O(1/t)$ gives $R_T \in O(k\log T)$.
Clearly unoptimal: !TODO! Why?
\paragraph{UCB1}
Init $\hat\mu_i \leftarrow 0$; try all arms once.
Following $t \in [T-k]$ rounds: $UCB(i) \leftarrow \hat\mu_i+\sqrt{\frac{2\log t}{n_i}}$, pick $i_t \leftarrow \argmax_i UCB(i)$, observe $y_t$. Update $n_{i_t} \leftarrow n_{i_t}+1, \hat\mu_{i_t}\leftarrow\hat\mu_{i_t}+\frac{y_t-\hat\mu_{i_t}}{n_{i_t}}$.
% Maybe perf. of UCB1 ?
\paragraph{Contextual bandits}
Round $t$: Obs.\ \emph{context} $\bm z_t \in\mathcal Z \subseteq \R^d$; \emph{recommend} $\bm x_t \in \mathcal A_t$.
Reward $y_t = f(\bm x_t,\bm z_t) + \eps_t$.
Regret $r_t = \max_{\bm x}f(\bm x, \bm z_t) - f(\bm x_t,\bm z_t)$.
Often $f(\bm x,\bm z) = \bm w_{\bm x}^T\bm z$ linear.

\paragraph{Idea behind LinUCB}
Estimate $\hat{\bm w}_{i} = \argmin_{\bm w}\sum_{t=1}^{m}(y_t-\bm w^T\bm z_t) + ||\bm w||_2^2$.
Closed form: $\hat{\bm w}_{i} = M_{i}^{-1}D_{i}^T\bm y_{i}$,
$M_{i} = D_{i}^TD_{i} + I$,
$D_{i} = [\bm z_1|\dots|\bm z_m], \bm y_{i} = (y_1|\dots|y_m)^T$.
\subparagraph{Confidence} If $\alpha = 1 + \sqrt{\ln(\frac{2}{\delta})/2}$: \newline
$\Pr\left [|\hat{\bm w}_{i}^T\bm z_t - \bm w_{i}^T\bm z_t| \leq \alpha\sqrt{\bm z_t^T M_{i}^{-1}\bm z_t}\right ] \geq 1-\delta$.

\paragraph{LinUCB (Algorithm)}
For $t = [T]$ receive \emph{action set} $A_t$ and features $\bm z_t$. For all $\bm x \in A_t$: if $\bm x$ new, set $M_{\bm x} \leftarrow \mathbb{I}$ and $b_x \leftarrow 0$;
set $\hat w_{\bm x} \leftarrow M_{\bm x}^{-1}b_{\bm x}$;
set $UCB_{\bm x} \leftarrow \hat w_{\bm x}^T \bm z_t + \alpha\sqrt{\bm z_t^T M_{\bm x}^{-1} \bm z_t}$.
Recommend action $\bm x_t = \argmax_{\bm x \in A_t} UCB_{\bm x}$; observe $y_t$.
Set $M_{\bm x} \leftarrow M_{\bm x} + \bm z_t \bm z_t^T$ and $b_{\bm x} \leftarrow b_{\bm x} + y_t \bm z_t$.

\paragraph{Hybrid Model}
$y_t = \bm w_i^T \bm z_t + \beta^T \phi(\bm x_i,\bm z_t) + \eps_t$
captures sep.\ and shared effects.

\paragraph{Rejection Sampling}
Evaluate bandit: For $t \in \N$ read log $(\bm x_1^{(t)},\dots,\bm x_k^{(t)},\bm z_t,a_t,y_t)$.
Pick $a'_t$ by algo. If $a'_t = a_t$ feed $y_t$ to algo., else ignore line. Stop after $T$ feedbacks.

\section{Submodularity}
$F:2^V \rightarrow \R$ \emph{subm.} iff $F(A \cup \{s \})-F(A) \geq F(B\cup \{s\})-F(B)$,\\
$\forall A \subseteq B \subseteq V, s \in V$ with $s \notin B$.

\paragraph{Closedness of non-negative linear combination:}
If $F_{1,\dots,m}(A)$ subm.\ then $\lambda_i>0:$ $F'(A) := \sum_i \lambda_i F_i(A)$ subm.

\paragraph{Other closedness:} $F(S)$ is subm.\ on $V$ and $W\subseteq V$. \emph{Restriction} $F(S \cap W)$. \emph{Conditioning}  $F(S \cup W)$, \emph{Reflection} $F(V \setminus S)$, are submodular.

\paragraph{Marginal gain:}
$\Delta_F(s|A)=F(\{s\} \cup A)-F(A)$

\paragraph{Greedy algo:}
In round $i+1$, previously picked $A_i=\{s_1,\dots,s_i\}$; pick $s_{i+1} = \argmax_{s} \Delta_F(s|A_i) = \argmax_{s} (F(\{s\}\cup A_i)-F(A_i))$.

\paragraph{Lazy Greedy:}
\emph{Observation:} Submodularity implies $\Delta(s|A_i) \geq \Delta(s|A_{i+1})$.
Algo.: $A_0 \leftarrow \{\}$; first iteration as usual.
Then keep ordered list of $\Delta_i$ from prev.\ iteration.
For $i \in [k]$ do: $\Delta_i=F(A_{i-1}\cup \{s^*\})$, $s^*=\argmax_s\Delta_F(s|A_{i-1})$ (\emph{top element}).
If $s^*$ is still top, then $A_i \leftarrow A_{i-1}\cup \{s^*\}$ else resort and pick top element (as in Greedy). 


\section{Tips}

$\sum{||y_i - w^Tx_i||^2} = \sum{(y_i - w^Tx_i)^T(y_i - w^Tx_i)} = (y - Xw)^T(y-Xw)$

\paragraph{Showing active learning needs n labels} Assume points are all distinct and algo has returned $-1$ for the first $n-1$ labels. Choose x s.t. algo can not distinguish between $-1,+1$.

\paragraph{Sublinear strategy approach} Sort and rename points. Cleverly search witnesses (points determining interval).

\paragraph{Deriving Dual} Start with quadratic. Rewrite constraints $c_{1,i},c_{2,i}$ s.t. $c_i(w)\geq0$. Get Lagrangian $L(w,\xi,\alpha,\gamma )$: $f(w) - \sum_i{\alpha_ic_{1,i}} - \sum_i{\gamma_ic_{2,i}}$. Set derivatives $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial \xi_i}$ to $0$ and get $w$, $C$. Insert into $L(w)$. Result is max. qp with constraint $\alpha_i \in [0,C]$

\end{multicols}


\end{document}
